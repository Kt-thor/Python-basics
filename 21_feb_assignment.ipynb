{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7ab45a1c-8ef6-4cca-a98e-786234e57c7b",
   "metadata": {},
   "source": [
    "Q1. What is Web Scraping? Why is it Used? Give three areas where Web Scraping is used to get data.\n",
    "\n",
    "\n",
    "Web scraping is a process of automating the extraction of data in an efficient and fast way. With the help of web scraping, you can extract data from any website, no matter how large is the data, on your computer. Moreover, websites may have data that you cannot copy and paste.\n",
    "\n",
    "1 . We can use web scrapping over crypto prices predicting sites\n",
    "\n",
    "2 . we can use to check live cricket score by scrapping\n",
    "\n",
    "3 . If we want information about  products review over any commercial product selling wesites"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e857c8f-1615-4d35-ba0c-1e1d737e6fc8",
   "metadata": {},
   "source": [
    "Q2. What are the different methods used for Web Scraping?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d807f55-03ac-4cdb-82a0-02d0f9ea63ad",
   "metadata": {},
   "source": [
    "Browser extensions Web Scrapers\n",
    "\n",
    "Cloud Web Scrapers\n",
    "\n",
    "Self-built Web Scrapers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef6c8fe0-cd2c-415d-bb7c-30f16dc5884c",
   "metadata": {},
   "source": [
    "Q3. What is Beautiful Soup? Why is it used?\n",
    "\n",
    "Beautiful Soup provides simple methods for navigating, searching, and modifying a parse tree in HTML, XML files. It transforms a complex HTML document into a tree of Python objects. It also automatically converts the document to Unicode, so you don't have to think about encoding"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff75de30-b4e5-4113-84d8-fbf8eaea681b",
   "metadata": {},
   "source": [
    "Q4. Why is flask used in this Web Scraping project?\n",
    "\n",
    "Flask is used to create the REST API for the project web scrapping.\n",
    "Flask is a lightweight framework to build websites. We'll use this to parse our collected data and display it as HTML in a new HTML file. The requests module allows us to send http requests to the website we want to scrape."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6dfe4dcb-1cd9-41c5-9c33-a570bb542918",
   "metadata": {},
   "source": [
    "Q5. Write the names of AWS services used in this project. Also, explain the use of each service.\n",
    "\n",
    "We used AWS pipeline and beanstck services for deployment\n",
    "\n",
    "1 : AWS pipeline : It connects the github repo of project to beanstack.\n",
    "\n",
    "2: Beanstack : it provide environment like a system having RAM,Secondry memory,Processors to execute the application got from pipeline and return the result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0340360e-8749-4272-b54a-a92f9749cf39",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
